# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
#
# This source code is licensed under the BSD license found in the
# LICENSE file in the root directory of this source tree.


import os
from typing import Any, Dict

import torch
import triton

from xformers.benchmarks.utils import TestCase, pretty_plot, pretty_print
from xformers.components.attention.attention_mask import AttentionMask
from xformers.components.attention.core import scaled_dot_product_attention

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

SHAPES = [
    (8, 128, 2096),
    (8, 1024, 256),
    (12, 512, 1024),
    (128, 128, 512),
    (8, 2048, 4096),
    (16, 1024, 5120),
    (512, 128, 2560),
]

BLOCK_SIZES = [128]
N_HEADS = [8, 32]


def bench_blocksparse_compare(backward: bool):
    device = torch.device("cuda")
    bw = "+bw" if backward else ""
    use_amp = True
    _use_cuda = True

    for dtype 